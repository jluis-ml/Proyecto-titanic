{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de los modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                             ConfusionMatrixDisplay, precision_recall_fscore_support,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedGroupKFold, train_test_split, cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:alembic.runtime.migration:Context impl SQLiteImpl.\n",
      "INFO:alembic.runtime.migration:Will assume non-transactional DDL.\n",
      "2024/05/03 13:16:02 INFO mlflow.tracking.fluent: Experiment with name 'Suvirved_titanic' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///d:/Repositorio/Proyecto-titanic/mlruns/1', creation_time=1714756562262, experiment_id='1', last_update_time=1714756562262, lifecycle_stage='active', name='Suvirved_titanic', tags={}>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"Suvirved_titanic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_name=str, path=\"D:/Repositorio/Proyecto-titanic/data_processed\")->pd.DataFrame:\n",
    "\n",
    "    #This function read the csv file\n",
    "    return pd.read_csv(os.path.join(path, file_name))\n",
    "\n",
    "def data_transform(df: pd.DataFrame):\n",
    "    #Thhis function transform the data into X and y\n",
    "    X = df[[\"Pclass\", \"Sex\", \"Age\", \"Siblings/Spouses Aboard\", \"Parents/Children Aboard\", \"Fare\"]]\n",
    "    y = df[\"Survived\"]\n",
    "    return X, y\n",
    "\n",
    "def one_hot_encoding(data, columns):\n",
    "    #This function apply one hot encoding\n",
    "    data_decoded = pd.get_dummies(data, columns=columns, drop_first=True)\n",
    "    return data_decoded.astype(int)\n",
    "\n",
    "def save_pickle(data, filename) -> None:\n",
    "    \"\"\"\n",
    "    This function saves the data in a pickle file\n",
    "    Args:\n",
    "        data (object): data to save\n",
    "        filename (str): filename\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(\"D:/Repositorio/Proyecto-titanic\", \"data_processed\", f\"{filename}.pkl\")\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "        \n",
    "def split_train_test(\n",
    "    X: np.array, y: pd.Series, test_size: float = 0.3, random_state: int = 42\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    This function splits the data into train and test\n",
    "    Args:\n",
    "      X_tfidf (np.array): array with the vectorized data\n",
    "      y (pd.Series): series with the labels\n",
    "      test_size (float): test size\n",
    "      random_state (int): random state\n",
    "    Returns:\n",
    "      X_train (np.array): array with the vectorized data for train\n",
    "      X_test (np.array): array with the vectorized data for test\n",
    "      y_train (pd.Series): series with the labels for train\n",
    "      y_test (pd.Series): series with the labels for test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    save_pickle((X_train, y_train), \"train\")\n",
    "    save_pickle((X_test, y_test),  \"test\")\n",
    "    logger.info(\"data saved successfully in pickle files\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def display_classification_report(\n",
    "    model: object,\n",
    "    name_model: str,\n",
    "    developer: str,\n",
    "    X_train: np.array,\n",
    "    X_test: np.array,\n",
    "    y_train: np.array,\n",
    "    y_test: np.array,\n",
    "    use_cv = False\n",
    "):\n",
    "    \"\"\"This function display the classification report\n",
    "    Args:\n",
    "      model (object): model\n",
    "      name_model (str): name of the model\n",
    "      developer (str): developer name\n",
    "      X_train (np.array): array with the vectorized data for train\n",
    "      X_test (np.array): array with the vectorized data for test\n",
    "      y_train (pd.Series): series with the labels for train\n",
    "      y_test (pd.Series): series with the labels for test\n",
    "     Returns:\n",
    "      metric (list): list with the metrics\"\"\"\n",
    "    \n",
    "    # star experiment in mlflow\n",
    "    with mlflow.start_run(run_name=name_model):\n",
    "        mlflow.log_param(\"model\", name_model)\n",
    "        mlflow.log_param(\"developer\", developer)\n",
    "        # empty list to store the metrics and then tracking them in mlflow\n",
    "        metric = []\n",
    "        y_train_pred_proba = model.predict_proba(X_train)\n",
    "        y_test_pred_proba = model.predict_proba(X_test)\n",
    "        roc_auc_score_train = round(\n",
    "            roc_auc_score(\n",
    "                y_train_flat, y_train_pred_proba, average=\"weighted\", multi_class=\"ovr\"\n",
    "            ),\n",
    "            2,\n",
    "        )\n",
    "        roc_auc_score_test = round(\n",
    "            roc_auc_score(\n",
    "                y_test, y_test_pred_proba, average=\"weighted\", multi_class=\"ovr\"\n",
    "            ),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "        logger.info(\"ROC AUC Score Train:\", roc_auc_score_train)\n",
    "        logger.info(\"ROC AUC Score Test:\", roc_auc_score_test)\n",
    "        \n",
    "        # adding the metrics to the list\n",
    "        metric.extend([roc_auc_score_train, roc_auc_score_test])\n",
    "\n",
    "        mlflow.log_metric(\"roc_auc_train\", roc_auc_score_train)\n",
    "        mlflow.log_metric(\"roc_auc_test\", roc_auc_score_test)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        (\n",
    "            precision_train,\n",
    "            recall_train,\n",
    "            fscore_train,\n",
    "            support_train,\n",
    "        ) = precision_recall_fscore_support(y_train, y_train_pred, average=\"weighted\")\n",
    "        (\n",
    "            precision_test,\n",
    "            recall_test,\n",
    "            fscore_test,\n",
    "            support_test,\n",
    "        ) = precision_recall_fscore_support(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "        mlflow.log_metric(\"precision_train\", precision_train)\n",
    "        mlflow.log_metric(\"precision_test\", precision_test)\n",
    "        mlflow.log_metric(\"recall_train\", recall_train)\n",
    "        mlflow.log_metric(\"recall_test\", recall_test)\n",
    "        \n",
    "        try:\n",
    "            if use_cv:\n",
    "                best_params = model.best_params_\n",
    "            else:\n",
    "                best_params = model.get_params()\n",
    "            mlflow.log_params(best_params)\n",
    "\n",
    "        except AttributeError as e:\n",
    "            logger.info(f\"Error: {e}\")\n",
    "\n",
    "        mlflow.sklearn.log_model(model, f\"model_{name_model}\")\n",
    "\n",
    "        acc_score_train = round(accuracy_score(y_train, y_train_pred), 2)\n",
    "        acc_score_test = round(accuracy_score(y_test, y_test_pred), 2)\n",
    "\n",
    "        metric.extend(\n",
    "            [\n",
    "                acc_score_train,\n",
    "                acc_score_test,\n",
    "                round(precision_train, 2),\n",
    "                round(precision_test, 2),\n",
    "                round(recall_train, 2),\n",
    "                round(recall_test, 2),\n",
    "                round(fscore_train, 2),\n",
    "                round(fscore_test, 2),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"Train Accuracy: \", acc_score_train)\n",
    "        print(\"Test Accuracy: \", acc_score_test)\n",
    "\n",
    "        model_report_train = classification_report(y_train, y_train_pred)\n",
    "        model_report_test = classification_report(y_test, y_test_pred)\n",
    "\n",
    "        print(\"Classification Report for Train:\\n\", model_report_train)\n",
    "        print(\"Classification Report for Test:\\n\", model_report_test)\n",
    "\n",
    "        # Plot the confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_test_pred)\n",
    "        cmp = ConfusionMatrixDisplay(cm, display_labels=model.classes_)\n",
    "        cmp.plot(ax=ax)\n",
    "\n",
    "        plt.xticks(rotation=80)\n",
    "        plt.show()\n",
    "\n",
    "        mlflow.sklearn.log_model(model, f\"models/{name_model}\")\n",
    "\n",
    "        return metric\n",
    "    \n",
    "    \n",
    "def grid_search(model, folds, params, scoring):\n",
    "    \"\"\"This function perform a grid search\n",
    "    Args:\n",
    "        model (object): model\n",
    "        folds (int): number of folds\n",
    "        params (dict): dictionary with the parameters\n",
    "        scoring (str): scoring\n",
    "    Returns:\n",
    "        grid_search (object): grid search\n",
    "    \"\"\"\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, cv=folds, param_grid=params, scoring=scoring, n_jobs=-1, verbose=1\n",
    "    )\n",
    "    return grid_search\n",
    "\n",
    "def print_best_score_params(model):\n",
    "    \"\"\"This functions print best score and best hyperparameters for baselines models\n",
    "    Args:\n",
    "        model (object): model\n",
    "    Returns:\n",
    "        None\"\"\"\n",
    "    print(\"Best Score: \", model.best_score_)\n",
    "    print(\"Best Hyperparameters: \", model.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:data saved successfully in pickle files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 8)\n",
      "(887, 6)\n",
      "(887,)\n",
      "(620, 6)\n",
      "(620,)\n"
     ]
    }
   ],
   "source": [
    "data_titamic = read_csv(\"titanic.csv\")\n",
    "X, y = data_transform(data_titamic)\n",
    "X = one_hot_encoding(X, [\"Sex\"])\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, y)\n",
    "\n",
    "print(data_titamic.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (887, 6)\n",
      "X_train (620, 6)\n",
      "y_train (620,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X\",X.shape)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "display_classification_report(model=mnb, \n",
    "                              name_model=\"MultinomialNB\", \n",
    "                              developer=\"Jose Luis\", \n",
    "                              X_train=X_train, \n",
    "                              X_test=X_test, \n",
    "                              y_train=y_train, \n",
    "                              y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
